{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#1754884 record,1053282 with coupon_id,9738 coupon. date_received:20160101~20160615,date:20160101~20160630, 539438 users, 8415 merchants\n",
    "off_train = pd.read_csv('data/ccf_offline_stage1_train.csv',header=None)\n",
    "off_train.columns = ['user_id','merchant_id','coupon_id','discount_rate','distance','date_received','date']\n",
    "#2050 coupon_id. date_received:20160701~20160731, 76309 users(76307 in trainset, 35965 in online_trainset), 1559 merchants(1558 in trainset)\n",
    "off_test = pd.read_csv('data/ccf_offline_stage1_test_revised.csv',header=None)\n",
    "off_test.columns = ['user_id','merchant_id','coupon_id','discount_rate','distance','date_received']\n",
    "#11429826 record(872357 with coupon_id),762858 user(267448 in off_train)\n",
    "on_train = pd.read_csv('data/ccf_online_stage1_train.csv',header=None)\n",
    "on_train.columns = ['user_id','merchant_id','action','coupon_id','discount_rate','date_received','date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset3 = off_test\n",
    "feature3 = off_train[((off_train.date>='20160315')&(off_train.date<='20160630'))|((off_train.date=='null')&(off_train.date_received>='20160315')&(off_train.date_received<='20160630'))]\n",
    "dataset2 = off_train[(off_train.date_received>='20160515')&(off_train.date_received<='20160615')]\n",
    "feature2 = off_train[(off_train.date>='20160201')&(off_train.date<='20160514')|((off_train.date=='null')&(off_train.date_received>='20160201')&(off_train.date_received<='20160514'))]\n",
    "dataset1 = off_train[(off_train.date_received>='20160414')&(off_train.date_received<='20160514')]\n",
    "feature1 = off_train[(off_train.date>='20160101')&(off_train.date<='20160413')|((off_train.date=='null')&(off_train.date_received>='20160101')&(off_train.date_received<='20160413'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\pandas\\core\\generic.py:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116204, 11)\n"
     ]
    }
   ],
   "source": [
    "#for dataset3\n",
    "t = dataset3[['user_id']]\n",
    "t['this_month_user_receive_all_coupon_count'] = 1\n",
    "t = t.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t1 = dataset3[['user_id','coupon_id']]\n",
    "t1['this_month_user_receive_same_coupon_count'] = 1\n",
    "t1 = t1.groupby(['user_id','coupon_id']).agg('sum').reset_index()\n",
    "\n",
    "t2 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t2.date_received = t2.date_received.astype('str')\n",
    "t2 = t2.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t2['receive_number'] = t2.date_received.apply(lambda s:len(s.split(':')))\n",
    "t2 = t2[t2.receive_number>1]\n",
    "t2['max_date_received'] = t2.date_received.apply(lambda s:max([int(d) for d in s.split(':')]))\n",
    "t2['min_date_received'] = t2.date_received.apply(lambda s:min([int(d) for d in s.split(':')]))\n",
    "t2 = t2[['user_id','coupon_id','max_date_received','min_date_received']]\n",
    "\n",
    "t3 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t3 = pd.merge(t3,t2,on=['user_id','coupon_id'],how='left')\n",
    "t3['this_month_user_receive_same_coupon_lastone'] = t3.max_date_received - t3.date_received\n",
    "t3['this_month_user_receive_same_coupon_firstone'] = t3.date_received - t3.min_date_received\n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "        \n",
    "t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)\n",
    "t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)\n",
    "t3 = t3[['user_id','coupon_id','date_received','this_month_user_receive_same_coupon_lastone','this_month_user_receive_same_coupon_firstone']]\n",
    "\n",
    "t4 = dataset3[['user_id','date_received']]\n",
    "t4['this_day_user_receive_all_coupon_count'] = 1\n",
    "t4 = t4.groupby(['user_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t5 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t5['this_day_user_receive_same_coupon_count'] = 1\n",
    "t5 = t5.groupby(['user_id','coupon_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t6 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t6.date_received = t6.date_received.astype('str')\n",
    "t6 = t6.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t6.rename(columns={'date_received':'dates'},inplace=True)\n",
    "\n",
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "        \n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "    \n",
    "\n",
    "t7 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t7 = pd.merge(t7,t6,on=['user_id','coupon_id'],how='left')\n",
    "t7['date_received_date'] = t7.date_received.astype('str') + '-' + t7.dates\n",
    "t7['day_gap_before'] = t7.date_received_date.apply(get_day_gap_before)\n",
    "t7['day_gap_after'] = t7.date_received_date.apply(get_day_gap_after)\n",
    "t7 = t7[['user_id','coupon_id','date_received','day_gap_before','day_gap_after']]\n",
    "\n",
    "other_feature3 = pd.merge(t1,t,on='user_id')\n",
    "other_feature3 = pd.merge(other_feature3,t3,on=['user_id','coupon_id'])\n",
    "other_feature3 = pd.merge(other_feature3,t4,on=['user_id','date_received'])\n",
    "other_feature3 = pd.merge(other_feature3,t5,on=['user_id','coupon_id','date_received'])\n",
    "other_feature3 = pd.merge(other_feature3,t7,on=['user_id','coupon_id','date_received'])\n",
    "other_feature3.to_csv('data/other_feature3.csv',index=None)\n",
    "print(other_feature3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\pandas\\core\\generic.py:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262240, 11)\n"
     ]
    }
   ],
   "source": [
    "#for dataset2\n",
    "t = dataset2[['user_id']]\n",
    "t['this_month_user_receive_all_coupon_count'] = 1\n",
    "t = t.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t1 = dataset2[['user_id','coupon_id']]\n",
    "t1['this_month_user_receive_same_coupon_count'] = 1\n",
    "t1 = t1.groupby(['user_id','coupon_id']).agg('sum').reset_index()\n",
    "\n",
    "t2 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t2.date_received = t2.date_received.astype('str')\n",
    "t2 = t2.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t2['receive_number'] = t2.date_received.apply(lambda s:len(s.split(':')))\n",
    "t2 = t2[t2.receive_number>1]\n",
    "t2['max_date_received'] = t2.date_received.apply(lambda s:max([int(d) for d in s.split(':')]))\n",
    "t2['min_date_received'] = t2.date_received.apply(lambda s:min([int(d) for d in s.split(':')]))\n",
    "t2 = t2[['user_id','coupon_id','max_date_received','min_date_received']]\n",
    "\n",
    "t3 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t3 = pd.merge(t3,t2,on=['user_id','coupon_id'],how='left')\n",
    "t3['this_month_user_receive_same_coupon_lastone'] = t3.max_date_received - t3.date_received.astype('int')\n",
    "t3['this_month_user_receive_same_coupon_firstone'] = t3.date_received.astype('int') - t3.min_date_received\n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "        \n",
    "t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)\n",
    "t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)\n",
    "t3 = t3[['user_id','coupon_id','date_received','this_month_user_receive_same_coupon_lastone','this_month_user_receive_same_coupon_firstone']]\n",
    "\n",
    "t4 = dataset2[['user_id','date_received']]\n",
    "t4['this_day_user_receive_all_coupon_count'] = 1\n",
    "t4 = t4.groupby(['user_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t5 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t5['this_day_user_receive_same_coupon_count'] = 1\n",
    "t5 = t5.groupby(['user_id','coupon_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t6 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t6.date_received = t6.date_received.astype('str')\n",
    "t6 = t6.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t6.rename(columns={'date_received':'dates'},inplace=True)\n",
    "\n",
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "        \n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "    \n",
    "\n",
    "t7 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t7 = pd.merge(t7,t6,on=['user_id','coupon_id'],how='left')\n",
    "t7['date_received_date'] = t7.date_received.astype('str') + '-' + t7.dates\n",
    "t7['day_gap_before'] = t7.date_received_date.apply(get_day_gap_before)\n",
    "t7['day_gap_after'] = t7.date_received_date.apply(get_day_gap_after)\n",
    "t7 = t7[['user_id','coupon_id','date_received','day_gap_before','day_gap_after']]\n",
    "\n",
    "other_feature2 = pd.merge(t1,t,on='user_id')\n",
    "other_feature2 = pd.merge(other_feature2,t3,on=['user_id','coupon_id'])\n",
    "other_feature2 = pd.merge(other_feature2,t4,on=['user_id','date_received'])\n",
    "other_feature2 = pd.merge(other_feature2,t5,on=['user_id','coupon_id','date_received'])\n",
    "other_feature2 = pd.merge(other_feature2,t7,on=['user_id','coupon_id','date_received'])\n",
    "other_feature2.to_csv('data/other_feature2.csv',index=None)\n",
    "print(other_feature2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\pandas\\core\\generic.py:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139785, 11)\n"
     ]
    }
   ],
   "source": [
    "#for dataset1\n",
    "t = dataset1[['user_id']]\n",
    "t['this_month_user_receive_all_coupon_count'] = 1\n",
    "t = t.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t1 = dataset1[['user_id','coupon_id']]\n",
    "t1['this_month_user_receive_same_coupon_count'] = 1\n",
    "t1 = t1.groupby(['user_id','coupon_id']).agg('sum').reset_index()\n",
    "\n",
    "t2 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t2.date_received = t2.date_received.astype('str')\n",
    "t2 = t2.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t2['receive_number'] = t2.date_received.apply(lambda s:len(s.split(':')))\n",
    "t2 = t2[t2.receive_number>1]\n",
    "t2['max_date_received'] = t2.date_received.apply(lambda s:max([int(d) for d in s.split(':')]))\n",
    "t2['min_date_received'] = t2.date_received.apply(lambda s:min([int(d) for d in s.split(':')]))\n",
    "t2 = t2[['user_id','coupon_id','max_date_received','min_date_received']]\n",
    "\n",
    "t3 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t3 = pd.merge(t3,t2,on=['user_id','coupon_id'],how='left')\n",
    "t3['this_month_user_receive_same_coupon_lastone'] = t3.max_date_received - t3.date_received.astype('int')\n",
    "t3['this_month_user_receive_same_coupon_firstone'] = t3.date_received.astype('int') - t3.min_date_received\n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "        \n",
    "t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)\n",
    "t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)\n",
    "t3 = t3[['user_id','coupon_id','date_received','this_month_user_receive_same_coupon_lastone','this_month_user_receive_same_coupon_firstone']]\n",
    "\n",
    "t4 = dataset1[['user_id','date_received']]\n",
    "t4['this_day_user_receive_all_coupon_count'] = 1\n",
    "t4 = t4.groupby(['user_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t5 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t5['this_day_user_receive_same_coupon_count'] = 1\n",
    "t5 = t5.groupby(['user_id','coupon_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t6 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t6.date_received = t6.date_received.astype('str')\n",
    "t6 = t6.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t6.rename(columns={'date_received':'dates'},inplace=True)\n",
    "\n",
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "        \n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "    \n",
    "\n",
    "t7 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t7 = pd.merge(t7,t6,on=['user_id','coupon_id'],how='left')\n",
    "t7['date_received_date'] = t7.date_received.astype('str') + '-' + t7.dates\n",
    "t7['day_gap_before'] = t7.date_received_date.apply(get_day_gap_before)\n",
    "t7['day_gap_after'] = t7.date_received_date.apply(get_day_gap_after)\n",
    "t7 = t7[['user_id','coupon_id','date_received','day_gap_before','day_gap_after']]\n",
    "\n",
    "other_feature1 = pd.merge(t1,t,on='user_id')\n",
    "other_feature1 = pd.merge(other_feature1,t3,on=['user_id','coupon_id'])\n",
    "other_feature1 = pd.merge(other_feature1,t4,on=['user_id','date_received'])\n",
    "other_feature1 = pd.merge(other_feature1,t5,on=['user_id','coupon_id','date_received'])\n",
    "other_feature1 = pd.merge(other_feature1,t7,on=['user_id','coupon_id','date_received'])\n",
    "other_feature1.to_csv('data/other_feature1.csv',index=None)\n",
    "print(other_feature1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# coupon related feature   #############\n",
    "\"\"\"\n",
    "2.coupon related: \n",
    "      discount_rate. discount_man. discount_jian. is_man_jian\n",
    "      day_of_week,day_of_month. (date_received)\n",
    "\"\"\"\n",
    "def calc_discount_rate(s):\n",
    "    s =str(s)\n",
    "    s = s.split(':')\n",
    "    if len(s)==1:\n",
    "        return float(s[0])\n",
    "    else:\n",
    "        return 1.0-float(s[1])/float(s[0])\n",
    "\n",
    "def get_discount_man(s):\n",
    "    s =str(s)\n",
    "    s = s.split(':')\n",
    "    if len(s)==1:\n",
    "        return 'null'\n",
    "    else:\n",
    "        return int(s[0])\n",
    "        \n",
    "def get_discount_jian(s):\n",
    "    s =str(s)\n",
    "    s = s.split(':')\n",
    "    if len(s)==1:\n",
    "        return 'null'\n",
    "    else:\n",
    "        return int(s[1])\n",
    "\n",
    "def is_man_jian(s):\n",
    "    s =str(s)\n",
    "    s = s.split(':')\n",
    "    if len(s)==1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#dataset3\n",
    "dataset3['day_of_week'] = dataset3.date_received.astype('str').apply(lambda x:date(int(x[0:4]),int(x[4:6]),int(x[6:8])).weekday()+1)\n",
    "dataset3['day_of_month'] = dataset3.date_received.astype('str').apply(lambda x:int(x[6:8]))\n",
    "dataset3['days_distance'] = dataset3.date_received.astype('str').apply(lambda x:(date(int(x[0:4]),int(x[4:6]),int(x[6:8]))-date(2016,6,30)).days)\n",
    "dataset3['discount_man'] = dataset3.discount_rate.apply(get_discount_man)\n",
    "dataset3['discount_jian'] = dataset3.discount_rate.apply(get_discount_jian)\n",
    "dataset3['is_man_jian'] = dataset3.discount_rate.apply(is_man_jian)\n",
    "dataset3['discount_rate'] = dataset3.discount_rate.apply(calc_discount_rate)\n",
    "d = dataset3[['coupon_id']]\n",
    "d['coupon_count'] = 1\n",
    "d = d.groupby('coupon_id').agg('sum').reset_index()\n",
    "dataset3 = pd.merge(dataset3,d,on='coupon_id',how='left')\n",
    "dataset3.to_csv('data/coupon3_feature.csv',index=None)\n",
    "#dataset2\n",
    "dataset2['day_of_week'] = dataset2.date_received.astype('str').apply(lambda x:date(int(x[0:4]),int(x[4:6]),int(x[6:8])).weekday()+1)\n",
    "dataset2['day_of_month'] = dataset2.date_received.astype('str').apply(lambda x:int(x[6:8]))\n",
    "dataset2['days_distance'] = dataset2.date_received.astype('str').apply(lambda x:(date(int(x[0:4]),int(x[4:6]),int(x[6:8]))-date(2016,5,14)).days)\n",
    "dataset2['discount_man'] = dataset2.discount_rate.apply(get_discount_man)\n",
    "dataset2['discount_jian'] = dataset2.discount_rate.apply(get_discount_jian)\n",
    "dataset2['is_man_jian'] = dataset2.discount_rate.apply(is_man_jian)\n",
    "dataset2['discount_rate'] = dataset2.discount_rate.apply(calc_discount_rate)\n",
    "d = dataset2[['coupon_id']]\n",
    "d['coupon_count'] = 1\n",
    "d = d.groupby('coupon_id').agg('sum').reset_index()\n",
    "dataset2 = pd.merge(dataset2,d,on='coupon_id',how='left')\n",
    "dataset2.to_csv('data/coupon2_feature.csv',index=None)\n",
    "#dataset1\n",
    "dataset1['day_of_week'] = dataset1.date_received.astype('str').apply(lambda x:date(int(x[0:4]),int(x[4:6]),int(x[6:8])).weekday()+1)\n",
    "dataset1['day_of_month'] = dataset1.date_received.astype('str').apply(lambda x:int(x[6:8]))\n",
    "dataset1['days_distance'] = dataset1.date_received.astype('str').apply(lambda x:(date(int(x[0:4]),int(x[4:6]),int(x[6:8]))-date(2016,4,13)).days)\n",
    "dataset1['discount_man'] = dataset1.discount_rate.apply(get_discount_man)\n",
    "dataset1['discount_jian'] = dataset1.discount_rate.apply(get_discount_jian)\n",
    "dataset1['is_man_jian'] = dataset1.discount_rate.apply(is_man_jian)\n",
    "dataset1['discount_rate'] = dataset1.discount_rate.apply(calc_discount_rate)\n",
    "d = dataset1[['coupon_id']]\n",
    "d['coupon_count'] = 1\n",
    "d = d.groupby('coupon_id').agg('sum').reset_index()\n",
    "dataset1 = pd.merge(dataset1,d,on='coupon_id',how='left')\n",
    "dataset1.to_csv('data/coupon1_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "############# merchant related feature   #############\n",
    "\"\"\"\n",
    "1.merchant related: \n",
    "      total_sales. sales_use_coupon.  total_coupon\n",
    "      coupon_rate = sales_use_coupon/total_sales.  \n",
    "      transfer_rate = sales_use_coupon/total_coupon. \n",
    "      merchant_avg_distance,merchant_min_distance,merchant_max_distance of those use coupon\n",
    "\"\"\"\n",
    "\n",
    "#for dataset3\n",
    "merchant3 = feature3[['merchant_id','coupon_id','distance','date_received','date']]\n",
    "\n",
    "t = merchant3[['merchant_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = merchant3[merchant3.date!='null'][['merchant_id']]\n",
    "t1['total_sales'] = 1\n",
    "t1 = t1.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t2 = merchant3[(merchant3.date!='null')&(merchant3.coupon_id!='null')][['merchant_id']]\n",
    "t2['sales_use_coupon'] = 1\n",
    "t2 = t2.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t3 = merchant3[merchant3.coupon_id!='null'][['merchant_id']]\n",
    "t3['total_coupon'] = 1\n",
    "t3 = t3.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t4 = merchant3[(merchant3.date!='null')&(merchant3.coupon_id!='null')][['merchant_id','distance']]\n",
    "t4.replace('null',-1,inplace=True)\n",
    "t4.distance = t4.distance.astype('int')\n",
    "t4.replace(-1,np.nan,inplace=True)\n",
    "t5 = t4.groupby('merchant_id').agg('min').reset_index()\n",
    "t5.rename(columns={'distance':'merchant_min_distance'},inplace=True)\n",
    "\n",
    "t6 = t4.groupby('merchant_id').agg('max').reset_index()\n",
    "t6.rename(columns={'distance':'merchant_max_distance'},inplace=True)\n",
    "\n",
    "t7 = t4.groupby('merchant_id').agg('mean').reset_index()\n",
    "t7.rename(columns={'distance':'merchant_mean_distance'},inplace=True)\n",
    "\n",
    "t8 = t4.groupby('merchant_id').agg('median').reset_index()\n",
    "t8.rename(columns={'distance':'merchant_median_distance'},inplace=True)\n",
    "\n",
    "merchant3_feature = pd.merge(t,t1,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t2,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t3,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t5,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t6,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t7,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t8,on='merchant_id',how='left')\n",
    "merchant3_feature.sales_use_coupon = merchant3_feature.sales_use_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant3_feature['merchant_coupon_transfer_rate'] = merchant3_feature.sales_use_coupon.astype('float') / merchant3_feature.total_coupon\n",
    "merchant3_feature['coupon_rate'] = merchant3_feature.sales_use_coupon.astype('float') / merchant3_feature.total_sales\n",
    "merchant3_feature.total_coupon = merchant3_feature.total_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant3_feature.to_csv('data/merchant3_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#for dataset2\n",
    "merchant2 = feature2[['merchant_id','coupon_id','distance','date_received','date']]\n",
    "\n",
    "t = merchant2[['merchant_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = merchant2[merchant2.date!='null'][['merchant_id']]\n",
    "t1['total_sales'] = 1\n",
    "t1 = t1.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t2 = merchant2[(merchant2.date!='null')&(merchant2.coupon_id!='null')][['merchant_id']]\n",
    "t2['sales_use_coupon'] = 1\n",
    "t2 = t2.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t3 = merchant2[merchant2.coupon_id!='null'][['merchant_id']]\n",
    "t3['total_coupon'] = 1\n",
    "t3 = t3.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t4 = merchant2[(merchant2.date!='null')&(merchant2.coupon_id!='null')][['merchant_id','distance']]\n",
    "t4.replace('null',-1,inplace=True)\n",
    "t4.distance = t4.distance.astype('int')\n",
    "t4.replace(-1,np.nan,inplace=True)\n",
    "t5 = t4.groupby('merchant_id').agg('min').reset_index()\n",
    "t5.rename(columns={'distance':'merchant_min_distance'},inplace=True)\n",
    "\n",
    "t6 = t4.groupby('merchant_id').agg('max').reset_index()\n",
    "t6.rename(columns={'distance':'merchant_max_distance'},inplace=True)\n",
    "\n",
    "t7 = t4.groupby('merchant_id').agg('mean').reset_index()\n",
    "t7.rename(columns={'distance':'merchant_mean_distance'},inplace=True)\n",
    "\n",
    "t8 = t4.groupby('merchant_id').agg('median').reset_index()\n",
    "t8.rename(columns={'distance':'merchant_median_distance'},inplace=True)\n",
    "\n",
    "merchant2_feature = pd.merge(t,t1,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t2,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t3,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t5,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t6,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t7,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t8,on='merchant_id',how='left')\n",
    "merchant2_feature.sales_use_coupon = merchant2_feature.sales_use_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant2_feature['merchant_coupon_transfer_rate'] = merchant2_feature.sales_use_coupon.astype('float') / merchant2_feature.total_coupon\n",
    "merchant2_feature['coupon_rate'] = merchant2_feature.sales_use_coupon.astype('float') / merchant2_feature.total_sales\n",
    "merchant2_feature.total_coupon = merchant2_feature.total_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant2_feature.to_csv('data/merchant2_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#for dataset1\n",
    "merchant1 = feature1[['merchant_id','coupon_id','distance','date_received','date']]\n",
    "\n",
    "t = merchant1[['merchant_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = merchant1[merchant1.date!='null'][['merchant_id']]\n",
    "t1['total_sales'] = 1\n",
    "t1 = t1.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t2 = merchant1[(merchant1.date!='null')&(merchant1.coupon_id!='null')][['merchant_id']]\n",
    "t2['sales_use_coupon'] = 1\n",
    "t2 = t2.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t3 = merchant1[merchant1.coupon_id!='null'][['merchant_id']]\n",
    "t3['total_coupon'] = 1\n",
    "t3 = t3.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t4 = merchant1[(merchant1.date!='null')&(merchant1.coupon_id!='null')][['merchant_id','distance']]\n",
    "t4.replace('null',-1,inplace=True)\n",
    "t4.distance = t4.distance.astype('int')\n",
    "t4.replace(-1,np.nan,inplace=True)\n",
    "t5 = t4.groupby('merchant_id').agg('min').reset_index()\n",
    "t5.rename(columns={'distance':'merchant_min_distance'},inplace=True)\n",
    "\n",
    "t6 = t4.groupby('merchant_id').agg('max').reset_index()\n",
    "t6.rename(columns={'distance':'merchant_max_distance'},inplace=True)\n",
    "\n",
    "t7 = t4.groupby('merchant_id').agg('mean').reset_index()\n",
    "t7.rename(columns={'distance':'merchant_mean_distance'},inplace=True)\n",
    "\n",
    "t8 = t4.groupby('merchant_id').agg('median').reset_index()\n",
    "t8.rename(columns={'distance':'merchant_median_distance'},inplace=True)\n",
    "\n",
    "\n",
    "merchant1_feature = pd.merge(t,t1,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t2,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t3,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t5,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t6,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t7,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t8,on='merchant_id',how='left')\n",
    "merchant1_feature.sales_use_coupon = merchant1_feature.sales_use_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant1_feature['merchant_coupon_transfer_rate'] = merchant1_feature.sales_use_coupon.astype('float') / merchant1_feature.total_coupon\n",
    "merchant1_feature['coupon_rate'] = merchant1_feature.sales_use_coupon.astype('float') / merchant1_feature.total_sales\n",
    "merchant1_feature.total_coupon = merchant1_feature.total_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant1_feature.to_csv('data/merchant1_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "############# user related feature   #############\n",
    "\"\"\"\n",
    "3.user related: \n",
    "      count_merchant. \n",
    "      user_avg_distance, user_min_distance,user_max_distance. \n",
    "      buy_use_coupon. buy_total. coupon_received.\n",
    "      buy_use_coupon/coupon_received. \n",
    "      buy_use_coupon/buy_total\n",
    "      user_date_datereceived_gap\n",
    "      \n",
    "\"\"\"\n",
    "\n",
    "def get_user_date_datereceived_gap(s):\n",
    "    s = s.split(':')\n",
    "    return (date(int(s[0][0:4]),int(s[0][4:6]),int(s[0][6:8])) - date(int(s[1][0:4]),int(s[1][4:6]),int(s[1][6:8]))).days\n",
    "\n",
    "#for dataset3\n",
    "user3 = feature3[['user_id','merchant_id','coupon_id','discount_rate','distance','date_received','date']]\n",
    "\n",
    "t = user3[['user_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = user3[user3.date!='null'][['user_id','merchant_id']]\n",
    "t1.drop_duplicates(inplace=True)\n",
    "t1.merchant_id = 1\n",
    "t1 = t1.groupby('user_id').agg('sum').reset_index()\n",
    "t1.rename(columns={'merchant_id':'count_merchant'},inplace=True)\n",
    "\n",
    "t2 = user3[(user3.date!='null')&(user3.coupon_id!='null')][['user_id','distance']]\n",
    "t2.replace('null',-1,inplace=True)\n",
    "t2.distance = t2.distance.astype('int')\n",
    "t2.replace(-1,np.nan,inplace=True)\n",
    "t3 = t2.groupby('user_id').agg('min').reset_index()\n",
    "t3.rename(columns={'distance':'user_min_distance'},inplace=True)\n",
    "\n",
    "t4 = t2.groupby('user_id').agg('max').reset_index()\n",
    "t4.rename(columns={'distance':'user_max_distance'},inplace=True)\n",
    "\n",
    "t5 = t2.groupby('user_id').agg('mean').reset_index()\n",
    "t5.rename(columns={'distance':'user_mean_distance'},inplace=True)\n",
    "\n",
    "t6 = t2.groupby('user_id').agg('median').reset_index()\n",
    "t6.rename(columns={'distance':'user_median_distance'},inplace=True)\n",
    "\n",
    "t7 = user3[(user3.date!='null')&(user3.coupon_id!='null')][['user_id']]\n",
    "t7['buy_use_coupon'] = 1\n",
    "t7 = t7.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t8 = user3[user3.date!='null'][['user_id']]\n",
    "t8['buy_total'] = 1\n",
    "t8 = t8.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t9 = user3[user3.coupon_id!='null'][['user_id']]\n",
    "t9['coupon_received'] = 1\n",
    "t9 = t9.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t10 = user3[(user3.date_received!='null')&(user3.date!='null')][['user_id','date_received','date']]\n",
    "t10['user_date_datereceived_gap'] = t10.date + ':' + t10.date_received\n",
    "t10.user_date_datereceived_gap = t10.user_date_datereceived_gap.apply(get_user_date_datereceived_gap)\n",
    "t10 = t10[['user_id','user_date_datereceived_gap']]\n",
    "\n",
    "t11 = t10.groupby('user_id').agg('mean').reset_index()\n",
    "t11.rename(columns={'user_date_datereceived_gap':'avg_user_date_datereceived_gap'},inplace=True)\n",
    "t12 = t10.groupby('user_id').agg('min').reset_index()\n",
    "t12.rename(columns={'user_date_datereceived_gap':'min_user_date_datereceived_gap'},inplace=True)\n",
    "t13 = t10.groupby('user_id').agg('max').reset_index()\n",
    "t13.rename(columns={'user_date_datereceived_gap':'max_user_date_datereceived_gap'},inplace=True)\n",
    "\n",
    "\n",
    "user3_feature = pd.merge(t,t1,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t3,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t4,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t5,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t6,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t7,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t8,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t9,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t11,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t12,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t13,on='user_id',how='left')\n",
    "user3_feature.count_merchant = user3_feature.count_merchant.replace(np.nan,0)\n",
    "user3_feature.buy_use_coupon = user3_feature.buy_use_coupon.replace(np.nan,0)\n",
    "user3_feature['buy_use_coupon_rate'] = user3_feature.buy_use_coupon.astype('float') / user3_feature.buy_total.astype('float')\n",
    "user3_feature['user_coupon_transfer_rate'] = user3_feature.buy_use_coupon.astype('float') / user3_feature.coupon_received.astype('float')\n",
    "user3_feature.buy_total = user3_feature.buy_total.replace(np.nan,0)\n",
    "user3_feature.coupon_received = user3_feature.coupon_received.replace(np.nan,0)\n",
    "user3_feature.to_csv('data/user3_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#for dataset2\n",
    "user2 = feature2[['user_id','merchant_id','coupon_id','discount_rate','distance','date_received','date']]\n",
    "\n",
    "t = user2[['user_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = user2[user2.date!='null'][['user_id','merchant_id']]\n",
    "t1.drop_duplicates(inplace=True)\n",
    "t1.merchant_id = 1\n",
    "t1 = t1.groupby('user_id').agg('sum').reset_index()\n",
    "t1.rename(columns={'merchant_id':'count_merchant'},inplace=True)\n",
    "\n",
    "t2 = user2[(user2.date!='null')&(user2.coupon_id!='null')][['user_id','distance']]\n",
    "t2.replace('null',-1,inplace=True)\n",
    "t2.distance = t2.distance.astype('int')\n",
    "t2.replace(-1,np.nan,inplace=True)\n",
    "t3 = t2.groupby('user_id').agg('min').reset_index()\n",
    "t3.rename(columns={'distance':'user_min_distance'},inplace=True)\n",
    "\n",
    "t4 = t2.groupby('user_id').agg('max').reset_index()\n",
    "t4.rename(columns={'distance':'user_max_distance'},inplace=True)\n",
    "\n",
    "t5 = t2.groupby('user_id').agg('mean').reset_index()\n",
    "t5.rename(columns={'distance':'user_mean_distance'},inplace=True)\n",
    "\n",
    "t6 = t2.groupby('user_id').agg('median').reset_index()\n",
    "t6.rename(columns={'distance':'user_median_distance'},inplace=True)\n",
    "\n",
    "t7 = user2[(user2.date!='null')&(user2.coupon_id!='null')][['user_id']]\n",
    "t7['buy_use_coupon'] = 1\n",
    "t7 = t7.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t8 = user2[user2.date!='null'][['user_id']]\n",
    "t8['buy_total'] = 1\n",
    "t8 = t8.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t9 = user2[user2.coupon_id!='null'][['user_id']]\n",
    "t9['coupon_received'] = 1\n",
    "t9 = t9.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t10 = user2[(user2.date_received!='null')&(user2.date!='null')][['user_id','date_received','date']]\n",
    "t10['user_date_datereceived_gap'] = t10.date + ':' + t10.date_received\n",
    "t10.user_date_datereceived_gap = t10.user_date_datereceived_gap.apply(get_user_date_datereceived_gap)\n",
    "t10 = t10[['user_id','user_date_datereceived_gap']]\n",
    "\n",
    "t11 = t10.groupby('user_id').agg('mean').reset_index()\n",
    "t11.rename(columns={'user_date_datereceived_gap':'avg_user_date_datereceived_gap'},inplace=True)\n",
    "t12 = t10.groupby('user_id').agg('min').reset_index()\n",
    "t12.rename(columns={'user_date_datereceived_gap':'min_user_date_datereceived_gap'},inplace=True)\n",
    "t13 = t10.groupby('user_id').agg('max').reset_index()\n",
    "t13.rename(columns={'user_date_datereceived_gap':'max_user_date_datereceived_gap'},inplace=True)\n",
    "\n",
    "user2_feature = pd.merge(t,t1,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t3,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t4,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t5,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t6,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t7,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t8,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t9,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t11,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t12,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t13,on='user_id',how='left')\n",
    "user2_feature.count_merchant = user2_feature.count_merchant.replace(np.nan,0)\n",
    "user2_feature.buy_use_coupon = user2_feature.buy_use_coupon.replace(np.nan,0)\n",
    "user2_feature['buy_use_coupon_rate'] = user2_feature.buy_use_coupon.astype('float') / user2_feature.buy_total.astype('float')\n",
    "user2_feature['user_coupon_transfer_rate'] = user2_feature.buy_use_coupon.astype('float') / user2_feature.coupon_received.astype('float')\n",
    "user2_feature.buy_total = user2_feature.buy_total.replace(np.nan,0)\n",
    "user2_feature.coupon_received = user2_feature.coupon_received.replace(np.nan,0)\n",
    "user2_feature.to_csv('data/user2_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#for dataset1\n",
    "user1 = feature1[['user_id','merchant_id','coupon_id','discount_rate','distance','date_received','date']]\n",
    "\n",
    "t = user1[['user_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = user1[user1.date!='null'][['user_id','merchant_id']]\n",
    "t1.drop_duplicates(inplace=True)\n",
    "t1.merchant_id = 1\n",
    "t1 = t1.groupby('user_id').agg('sum').reset_index()\n",
    "t1.rename(columns={'merchant_id':'count_merchant'},inplace=True)\n",
    "\n",
    "t2 = user1[(user1.date!='null')&(user1.coupon_id!='null')][['user_id','distance']]\n",
    "t2.replace('null',-1,inplace=True)\n",
    "t2.distance = t2.distance.astype('int')\n",
    "t2.replace(-1,np.nan,inplace=True)\n",
    "t3 = t2.groupby('user_id').agg('min').reset_index()\n",
    "t3.rename(columns={'distance':'user_min_distance'},inplace=True)\n",
    "\n",
    "t4 = t2.groupby('user_id').agg('max').reset_index()\n",
    "t4.rename(columns={'distance':'user_max_distance'},inplace=True)\n",
    "\n",
    "t5 = t2.groupby('user_id').agg('mean').reset_index()\n",
    "t5.rename(columns={'distance':'user_mean_distance'},inplace=True)\n",
    "\n",
    "t6 = t2.groupby('user_id').agg('median').reset_index()\n",
    "t6.rename(columns={'distance':'user_median_distance'},inplace=True)\n",
    "\n",
    "t7 = user1[(user1.date!='null')&(user1.coupon_id!='null')][['user_id']]\n",
    "t7['buy_use_coupon'] = 1\n",
    "t7 = t7.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t8 = user1[user1.date!='null'][['user_id']]\n",
    "t8['buy_total'] = 1\n",
    "t8 = t8.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t9 = user1[user1.coupon_id!='null'][['user_id']]\n",
    "t9['coupon_received'] = 1\n",
    "t9 = t9.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t10 = user1[(user1.date_received!='null')&(user1.date!='null')][['user_id','date_received','date']]\n",
    "t10['user_date_datereceived_gap'] = t10.date + ':' + t10.date_received\n",
    "t10.user_date_datereceived_gap = t10.user_date_datereceived_gap.apply(get_user_date_datereceived_gap)\n",
    "t10 = t10[['user_id','user_date_datereceived_gap']]\n",
    "\n",
    "t11 = t10.groupby('user_id').agg('mean').reset_index()\n",
    "t11.rename(columns={'user_date_datereceived_gap':'avg_user_date_datereceived_gap'},inplace=True)\n",
    "t12 = t10.groupby('user_id').agg('min').reset_index()\n",
    "t12.rename(columns={'user_date_datereceived_gap':'min_user_date_datereceived_gap'},inplace=True)\n",
    "t13 = t10.groupby('user_id').agg('max').reset_index()\n",
    "t13.rename(columns={'user_date_datereceived_gap':'max_user_date_datereceived_gap'},inplace=True)\n",
    "\n",
    "user1_feature = pd.merge(t,t1,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t3,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t4,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t5,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t6,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t7,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t8,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t9,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t11,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t12,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t13,on='user_id',how='left')\n",
    "user1_feature.count_merchant = user1_feature.count_merchant.replace(np.nan,0)\n",
    "user1_feature.buy_use_coupon = user1_feature.buy_use_coupon.replace(np.nan,0)\n",
    "user1_feature['buy_use_coupon_rate'] = user1_feature.buy_use_coupon.astype('float') / user1_feature.buy_total.astype('float')\n",
    "user1_feature['user_coupon_transfer_rate'] = user1_feature.buy_use_coupon.astype('float') / user1_feature.coupon_received.astype('float')\n",
    "user1_feature.buy_total = user1_feature.buy_total.replace(np.nan,0)\n",
    "user1_feature.coupon_received = user1_feature.coupon_received.replace(np.nan,0)\n",
    "user1_feature.to_csv('data/user1_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "##################  user_merchant related feature #########################\n",
    "\n",
    "\"\"\"\n",
    "4.user_merchant:\n",
    "      times_user_buy_merchant_before. \n",
    "\"\"\"\n",
    "#for dataset3\n",
    "all_user_merchant = feature3[['user_id','merchant_id']]\n",
    "all_user_merchant.drop_duplicates(inplace=True)\n",
    "\n",
    "t = feature3[['user_id','merchant_id','date']]\n",
    "t = t[t.date!='null'][['user_id','merchant_id']]\n",
    "t['user_merchant_buy_total'] = 1\n",
    "t = t.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = feature3[['user_id','merchant_id','coupon_id']]\n",
    "t1 = t1[t1.coupon_id!='null'][['user_id','merchant_id']]\n",
    "t1['user_merchant_received'] = 1\n",
    "t1 = t1.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t1.drop_duplicates(inplace=True)\n",
    "\n",
    "t2 = feature3[['user_id','merchant_id','date','date_received']]\n",
    "t2 = t2[(t2.date!='null')&(t2.date_received!='null')][['user_id','merchant_id']]\n",
    "t2['user_merchant_buy_use_coupon'] = 1\n",
    "t2 = t2.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t2.drop_duplicates(inplace=True)\n",
    "\n",
    "t3 = feature3[['user_id','merchant_id']]\n",
    "t3['user_merchant_any'] = 1\n",
    "t3 = t3.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t3.drop_duplicates(inplace=True)\n",
    "\n",
    "t4 = feature3[['user_id','merchant_id','date','coupon_id']]\n",
    "t4 = t4[(t4.date!='null')&(t4.coupon_id=='null')][['user_id','merchant_id']]\n",
    "t4['user_merchant_buy_common'] = 1\n",
    "t4 = t4.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t4.drop_duplicates(inplace=True)\n",
    "\n",
    "user_merchant3 = pd.merge(all_user_merchant,t,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t1,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t2,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t3,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t4,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3.user_merchant_buy_use_coupon = user_merchant3.user_merchant_buy_use_coupon.replace(np.nan,0)\n",
    "user_merchant3.user_merchant_buy_common = user_merchant3.user_merchant_buy_common.replace(np.nan,0)\n",
    "user_merchant3['user_merchant_coupon_transfer_rate'] = user_merchant3.user_merchant_buy_use_coupon.astype('float') / user_merchant3.user_merchant_received.astype('float')\n",
    "user_merchant3['user_merchant_coupon_buy_rate'] = user_merchant3.user_merchant_buy_use_coupon.astype('float') / user_merchant3.user_merchant_buy_total.astype('float')\n",
    "user_merchant3['user_merchant_rate'] = user_merchant3.user_merchant_buy_total.astype('float') / user_merchant3.user_merchant_any.astype('float')\n",
    "user_merchant3['user_merchant_common_buy_rate'] = user_merchant3.user_merchant_buy_common.astype('float') / user_merchant3.user_merchant_buy_total.astype('float')\n",
    "user_merchant3.to_csv('data/user_merchant3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#for dataset2\n",
    "all_user_merchant = feature2[['user_id','merchant_id']]\n",
    "all_user_merchant.drop_duplicates(inplace=True)\n",
    "\n",
    "t = feature2[['user_id','merchant_id','date']]\n",
    "t = t[t.date!='null'][['user_id','merchant_id']]\n",
    "t['user_merchant_buy_total'] = 1\n",
    "t = t.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = feature2[['user_id','merchant_id','coupon_id']]\n",
    "t1 = t1[t1.coupon_id!='null'][['user_id','merchant_id']]\n",
    "t1['user_merchant_received'] = 1\n",
    "t1 = t1.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t1.drop_duplicates(inplace=True)\n",
    "\n",
    "t2 = feature2[['user_id','merchant_id','date','date_received']]\n",
    "t2 = t2[(t2.date!='null')&(t2.date_received!='null')][['user_id','merchant_id']]\n",
    "t2['user_merchant_buy_use_coupon'] = 1\n",
    "t2 = t2.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t2.drop_duplicates(inplace=True)\n",
    "\n",
    "t3 = feature2[['user_id','merchant_id']]\n",
    "t3['user_merchant_any'] = 1\n",
    "t3 = t3.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t3.drop_duplicates(inplace=True)\n",
    "\n",
    "t4 = feature2[['user_id','merchant_id','date','coupon_id']]\n",
    "t4 = t4[(t4.date!='null')&(t4.coupon_id=='null')][['user_id','merchant_id']]\n",
    "t4['user_merchant_buy_common'] = 1\n",
    "t4 = t4.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t4.drop_duplicates(inplace=True)\n",
    "\n",
    "user_merchant2 = pd.merge(all_user_merchant,t,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t1,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t2,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t3,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t4,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2.user_merchant_buy_use_coupon = user_merchant2.user_merchant_buy_use_coupon.replace(np.nan,0)\n",
    "user_merchant2.user_merchant_buy_common = user_merchant2.user_merchant_buy_common.replace(np.nan,0)\n",
    "user_merchant2['user_merchant_coupon_transfer_rate'] = user_merchant2.user_merchant_buy_use_coupon.astype('float') / user_merchant2.user_merchant_received.astype('float')\n",
    "user_merchant2['user_merchant_coupon_buy_rate'] = user_merchant2.user_merchant_buy_use_coupon.astype('float') / user_merchant2.user_merchant_buy_total.astype('float')\n",
    "user_merchant2['user_merchant_rate'] = user_merchant2.user_merchant_buy_total.astype('float') / user_merchant2.user_merchant_any.astype('float')\n",
    "user_merchant2['user_merchant_common_buy_rate'] = user_merchant2.user_merchant_buy_common.astype('float') / user_merchant2.user_merchant_buy_total.astype('float')\n",
    "user_merchant2.to_csv('data/user_merchant2.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#for dataset2\n",
    "all_user_merchant = feature1[['user_id','merchant_id']]\n",
    "all_user_merchant.drop_duplicates(inplace=True)\n",
    "\n",
    "t = feature1[['user_id','merchant_id','date']]\n",
    "t = t[t.date!='null'][['user_id','merchant_id']]\n",
    "t['user_merchant_buy_total'] = 1\n",
    "t = t.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = feature1[['user_id','merchant_id','coupon_id']]\n",
    "t1 = t1[t1.coupon_id!='null'][['user_id','merchant_id']]\n",
    "t1['user_merchant_received'] = 1\n",
    "t1 = t1.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t1.drop_duplicates(inplace=True)\n",
    "\n",
    "t2 = feature1[['user_id','merchant_id','date','date_received']]\n",
    "t2 = t2[(t2.date!='null')&(t2.date_received!='null')][['user_id','merchant_id']]\n",
    "t2['user_merchant_buy_use_coupon'] = 1\n",
    "t2 = t2.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t2.drop_duplicates(inplace=True)\n",
    "\n",
    "t3 = feature1[['user_id','merchant_id']]\n",
    "t3['user_merchant_any'] = 1\n",
    "t3 = t3.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t3.drop_duplicates(inplace=True)\n",
    "\n",
    "t4 = feature1[['user_id','merchant_id','date','coupon_id']]\n",
    "t4 = t4[(t4.date!='null')&(t4.coupon_id=='null')][['user_id','merchant_id']]\n",
    "t4['user_merchant_buy_common'] = 1\n",
    "t4 = t4.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t4.drop_duplicates(inplace=True)\n",
    "\n",
    "user_merchant1 = pd.merge(all_user_merchant,t,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t1,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t2,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t3,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t4,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1.user_merchant_buy_use_coupon = user_merchant1.user_merchant_buy_use_coupon.replace(np.nan,0)\n",
    "user_merchant1.user_merchant_buy_common = user_merchant1.user_merchant_buy_common.replace(np.nan,0)\n",
    "user_merchant1['user_merchant_coupon_transfer_rate'] = user_merchant1.user_merchant_buy_use_coupon.astype('float') / user_merchant1.user_merchant_received.astype('float')\n",
    "user_merchant1['user_merchant_coupon_buy_rate'] = user_merchant1.user_merchant_buy_use_coupon.astype('float') / user_merchant1.user_merchant_buy_total.astype('float')\n",
    "user_merchant1['user_merchant_rate'] = user_merchant1.user_merchant_buy_total.astype('float') / user_merchant1.user_merchant_any.astype('float')\n",
    "user_merchant1['user_merchant_common_buy_rate'] = user_merchant1.user_merchant_buy_common.astype('float') / user_merchant1.user_merchant_buy_total.astype('float')\n",
    "user_merchant1.to_csv('data/user_merchant1.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################  generate training and testing set ################\n",
    "def get_label(s):\n",
    "    s = s.split(':')\n",
    "    if s[0]=='null':\n",
    "        return 0\n",
    "    elif (date(int(s[0][0:4]),int(s[0][4:6]),int(s[0][6:8]))-date(int(s[1][0:4]),int(s[1][4:6]),int(s[1][6:8]))).days<=15:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(222261, 52)\n"
     ]
    }
   ],
   "source": [
    "coupon3 = pd.read_csv('data/coupon3_feature.csv')\n",
    "merchant3 = pd.read_csv('data/merchant3_feature.csv')\n",
    "user3 = pd.read_csv('data/user3_feature.csv')\n",
    "user_merchant3 = pd.read_csv('data/user_merchant3.csv')\n",
    "other_feature3 = pd.read_csv('data/other_feature3.csv')\n",
    "dataset3 = pd.merge(coupon3,merchant3,on='merchant_id',how='left')\n",
    "dataset3 = pd.merge(dataset3,user3,on='user_id',how='left')\n",
    "dataset3 = pd.merge(dataset3,user_merchant3,on=['user_id','merchant_id'],how='left')\n",
    "dataset3 = pd.merge(dataset3,other_feature3,on=['user_id','coupon_id','date_received'],how='left')\n",
    "dataset3.drop_duplicates(inplace=True)\n",
    "print(dataset3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset3.user_merchant_buy_total = dataset3.user_merchant_buy_total.replace(np.nan,0)\n",
    "dataset3.user_merchant_any = dataset3.user_merchant_any.replace(np.nan,0)\n",
    "dataset3.user_merchant_received = dataset3.user_merchant_received.replace(np.nan,0)\n",
    "dataset3['is_weekend'] = dataset3.day_of_week.apply(lambda x:1 if x in (6,7) else 0)\n",
    "weekday_dummies = pd.get_dummies(dataset3.day_of_week)\n",
    "weekday_dummies.columns = ['weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset3 = pd.concat([dataset3,weekday_dummies],axis=1)\n",
    "dataset3.drop(['merchant_id','day_of_week','coupon_count'],axis=1,inplace=True)\n",
    "dataset3 = dataset3.replace('null',np.nan)\n",
    "dataset3.to_csv('data/dataset3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492696, 53)\n"
     ]
    }
   ],
   "source": [
    "coupon2 = pd.read_csv('data/coupon2_feature.csv')\n",
    "merchant2 = pd.read_csv('data/merchant2_feature.csv')\n",
    "user2 = pd.read_csv('data/user2_feature.csv')\n",
    "user_merchant2 = pd.read_csv('data/user_merchant2.csv')\n",
    "other_feature2 = pd.read_csv('data/other_feature2.csv')\n",
    "dataset2 = pd.merge(coupon2,merchant2,on='merchant_id',how='left')\n",
    "dataset2 = pd.merge(dataset2,user2,on='user_id',how='left')\n",
    "dataset2 = pd.merge(dataset2,user_merchant2,on=['user_id','merchant_id'],how='left')\n",
    "dataset2 = pd.merge(dataset2,other_feature2,on=['user_id','coupon_id','date_received'],how='left')\n",
    "dataset2.drop_duplicates(inplace=True)\n",
    "print(dataset2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset2.user_merchant_buy_total = dataset2.user_merchant_buy_total.replace(np.nan,0)\n",
    "dataset2.user_merchant_any = dataset2.user_merchant_any.replace(np.nan,0)\n",
    "dataset2.user_merchant_received = dataset2.user_merchant_received.replace(np.nan,0)\n",
    "dataset2['is_weekend'] = dataset2.day_of_week.apply(lambda x:1 if x in (6,7) else 0)\n",
    "weekday_dummies = pd.get_dummies(dataset2.day_of_week)\n",
    "weekday_dummies.columns = ['weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset2 = pd.concat([dataset2,weekday_dummies],axis=1)\n",
    "dataset2['label'] = dataset2.date.astype('str') + ':' +  dataset2.date_received.astype('str')\n",
    "dataset2.label = dataset2.label.apply(get_label)\n",
    "dataset2.drop(['merchant_id','day_of_week','date','date_received','coupon_id','coupon_count'],axis=1,inplace=True)\n",
    "dataset2 = dataset2.replace('null',np.nan)\n",
    "dataset2.to_csv('data/dataset2.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255225, 53)\n"
     ]
    }
   ],
   "source": [
    "coupon1 = pd.read_csv('data/coupon1_feature.csv')\n",
    "merchant1 = pd.read_csv('data/merchant1_feature.csv')\n",
    "user1 = pd.read_csv('data/user1_feature.csv')\n",
    "user_merchant1 = pd.read_csv('data/user_merchant1.csv')\n",
    "other_feature1 = pd.read_csv('data/other_feature1.csv')\n",
    "dataset1 = pd.merge(coupon1,merchant1,on='merchant_id',how='left')\n",
    "dataset1 = pd.merge(dataset1,user1,on='user_id',how='left')\n",
    "dataset1 = pd.merge(dataset1,user_merchant1,on=['user_id','merchant_id'],how='left')\n",
    "dataset1 = pd.merge(dataset1,other_feature1,on=['user_id','coupon_id','date_received'],how='left')\n",
    "dataset1.drop_duplicates(inplace=True)\n",
    "print(dataset1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset1.user_merchant_buy_total = dataset1.user_merchant_buy_total.replace(np.nan,0)\n",
    "dataset1.user_merchant_any = dataset1.user_merchant_any.replace(np.nan,0)\n",
    "dataset1.user_merchant_received = dataset1.user_merchant_received.replace(np.nan,0)\n",
    "dataset1['is_weekend'] = dataset1.day_of_week.apply(lambda x:1 if x in (6,7) else 0)\n",
    "weekday_dummies = pd.get_dummies(dataset1.day_of_week)\n",
    "weekday_dummies.columns = ['weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset1 = pd.concat([dataset1,weekday_dummies],axis=1)\n",
    "dataset1['label'] = dataset1.date.astype('str') + ':' +  dataset1.date_received.astype('str')\n",
    "dataset1.label = dataset1.label.apply(get_label)\n",
    "dataset1.drop(['merchant_id','day_of_week','date','date_received','coupon_id','coupon_count'],axis=1,inplace=True)\n",
    "dataset1 = dataset1.replace('null',np.nan)\n",
    "dataset1.to_csv('data/dataset1.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
