{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc \n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只提取用户-商品特征，体现用户在购买选择时的选择，用于f12指标。\n",
    "u模型重点在于：用户是否买cate8商品。us模型重点在于：已知用户要买的前提下，在用户看过的商品中选取购买概率最大的那个商品。那么如何选取特征？重点为us 交互特征，s特征，b特征，ub特征，以及排序特征等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_2_path = \"data/JData_Action_201602.csv\"\n",
    "action_3_path = \"data/JData_Action_201603.csv\"\n",
    "action_4_path = \"data/JData_Action_201604.csv\"\n",
    "comment_path = \"data/JData_Comment.csv\"\n",
    "product_path = \"data/JData_Product.csv\"\n",
    "user_path = \"data/JData_User.csv\"\n",
    "\n",
    "comment_date = [\"2016-02-01\", \"2016-02-08\", \"2016-02-15\", \"2016-02-22\", \"2016-02-29\", \"2016-03-07\", \"2016-03-14\",\n",
    "                \"2016-03-21\", \"2016-03-28\",\n",
    "                \"2016-04-04\", \"2016-04-11\", \"2016-04-15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_differ_minutes(str1,str2):\n",
    "    t1=datetime.strptime(str1,'%Y-%m-%d %H:%M:%S')\n",
    "    t2=datetime.strptime(str2,'%Y-%m-%d %H:%M:%S')\n",
    "    return ((t1-t2).seconds)/60+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算相差的天数\n",
    "def diff_of_days1(day1,day2):\n",
    "    try:\n",
    "        return (pd.Timestamp(day1)-pd.Timestamp(day2)).days\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_differ_days(str1,str2):\n",
    "    t1=datetime.strptime(str1,'%Y-%m-%d %H:%M:%S')\n",
    "    t2=datetime.strptime(str2,'%Y-%m-%d %H:%M:%S')\n",
    "    return (t1-t2).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_differ_days_nyr(str1,str2):\n",
    "    t1=datetime.strptime(str1,'%Y-%m-%d')\n",
    "    t2=datetime.strptime(str2,'%Y-%m-%d')\n",
    "    return (t1-t2).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 相差的分钟数\n",
    "def diff_of_minutes(day1, day2):\n",
    "    d = {'1': 0, '2': 31, '3': 60, '4': 91}\n",
    "    try:\n",
    "        days = (d[day1[6]] + int(day1[8:10])) - (d[day2[6]] + int(day2[8:10]))\n",
    "        minutes = int(day2[11:13]) * 60 + int(day2[14:16])\n",
    "        return (days * 1440 - minutes)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 相差的小时数\n",
    "def diff_of_hours(day1, day2):\n",
    "    d = {'1': 0, '2': 31, '3': 60, '4': 91}\n",
    "    try:\n",
    "        days = (d[day1[6]] + int(day1[8:10])) - (d[day2[6]] + int(day2[8:10]))\n",
    "        hours = int(day2[11:13])\n",
    "        return (days * 24 - hours)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action2():\n",
    "    action2=pd.read_csv(action_2_path)\n",
    "    return action2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action3():\n",
    "    action3=pd.read_csv(action_3_path)\n",
    "    return action3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action4():\n",
    "    action4=pd.read_csv(action_4_path)\n",
    "    return action4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_actions_cate8(start_date,end_date):\n",
    "    action2=get_action2()\n",
    "    action3=get_action3()\n",
    "    action4=get_action4()\n",
    "    action=pd.concat([action2,action3,action4],axis=0)\n",
    "    result=action[(action.time>=start_date)&(action.time<end_date)&(action.cate==8)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_basic_product_feat():\n",
    "    dump_path = r'cache/us/product_feat.pkl'\n",
    "    if os.path.exists(dump_path):\n",
    "        product = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        product = pd.read_csv(product_path)\n",
    "        attr1_df = pd.get_dummies(product[\"a1\"], prefix=\"a1\")\n",
    "        attr2_df = pd.get_dummies(product[\"a2\"], prefix=\"a2\")\n",
    "        attr3_df = pd.get_dummies(product[\"a3\"], prefix=\"a3\")\n",
    "        product = pd.concat([product[['sku_id', 'cate', 'brand']], attr1_df, attr2_df, attr3_df], axis=1)\n",
    "        pickle.dump(product, open(dump_path, 'wb+'))\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#商品comment信息（评论数，差评率）\n",
    "def get_comments(end_date):\n",
    "    dump_path = r'cache/us/comments_feat_%s.pkl'% (end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        comments = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        comments = pd.read_csv(comment_path)\n",
    "        comment_date_end = end_date\n",
    "        comment_date_begin = comment_date[0]\n",
    "        for date in reversed(comment_date):\n",
    "            if date < comment_date_end:\n",
    "                comment_date_begin = date\n",
    "                break\n",
    "        comments = comments[comments.dt == comment_date_begin]\n",
    "        del comments['dt']\n",
    "        pickle.dump(comments, open(dump_path, 'wb+'))\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据集 features公共的 特征 如对6种行为的统计 如 user_id,brand,sku_id 对cate8的行为\n",
    "def get_public_single_time_features(key,start_date,end_date,d):\n",
    "    if key=='user_id':\n",
    "        tt='user'\n",
    "    elif key=='sku_id':\n",
    "        tt='sku'\n",
    "    else:\n",
    "        tt='brand'\n",
    "    dump_path = r'cache/us/%s_action_time_features_%s_%s.pkl' % (tt,start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        features = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        dataset=get_actions_cate8(start_date,end_date)\n",
    "        #6种行为统计+时间\n",
    "        features=None\n",
    "        if key=='user_id': \n",
    "            t=dataset[[key]]\n",
    "            t.drop_duplicates(inplace=True) #去重\n",
    "            #6种行为 type=4 用处非常小\n",
    "            for i in (1,2,3,5,6):\n",
    "                action=dataset[dataset.type==i][[key]]\n",
    "                action['%d_%s_type_%s_num'%(d,key[:1],str(i))]=1\n",
    "                action=action.groupby([key]).agg('sum').reset_index()\n",
    "                if features is None:\n",
    "                    features=pd.merge(t,action,on=[key],how='left')\n",
    "                else:\n",
    "                    features=pd.merge(features,action,on=[key],how='left')\n",
    "        elif key=='sku_id':\n",
    "            # 商品统计行为数\n",
    "            t=dataset[[key,'brand','cate']]\n",
    "            t.drop_duplicates(inplace=True) #去重\n",
    "            for i in (1,2,3,4,5,6):\n",
    "                action=dataset[dataset.type==i][[key]]\n",
    "                action['%d_%s_type_%s_num'%(d,key[:1],str(i))]=1\n",
    "                action=action.groupby([key]).agg('sum').reset_index()\n",
    "                if features is None:\n",
    "                    features=pd.merge(t,action,on=[key],how='left')\n",
    "                else:\n",
    "                    features=pd.merge(features,action,on=[key],how='left')\n",
    "            features=features.fillna(0)\n",
    "            if d==60:\n",
    "                for i in (1,2,3,4,5,6):\n",
    "                    #商品在同一品牌中的排序\n",
    "                    features['%d_%s_type_%s_num_b_rank'%(d,key[:1],str(i))]=features['%d_%s_type_%s_num'%(d,key[:1],str(i))].groupby(features['brand']).rank(ascending=False,method='min')\n",
    "                    #商品在cate8中的排序\n",
    "                    features['%d_%s_type_%s_num_c_rank'%(d,key[:1],str(i))]=features['%d_%s_type_%s_num'%(d,key[:1],str(i))].groupby(features['cate']).rank(ascending=False,method='min')\n",
    "            del features['cate']\n",
    "            del features['brand']\n",
    "        else:\n",
    "            #品牌统计行为数\n",
    "            t=dataset[[key,'cate']]\n",
    "            t.drop_duplicates(inplace=True) #去重\n",
    "            for i in (1,2,3,4,5,6):\n",
    "                action=dataset[dataset.type==i][[key]]\n",
    "                action['%d_%s_type_%s_num'%(d,key[:1],str(i))]=1\n",
    "                action=action.groupby([key]).agg('sum').reset_index()\n",
    "                if features is None:\n",
    "                    features=pd.merge(t,action,on=[key],how='left')\n",
    "                else:\n",
    "                    features=pd.merge(features,action,on=[key],how='left')\n",
    "            features=features.fillna(0)\n",
    "            if d==60:\n",
    "                for i in (1,2,3,4,5,6):\n",
    "                    #品牌在cate8中的排序\n",
    "                    features['%d_%s_type_%s_num_c_rank'%(d,key[:1],str(i))]=features['%d_%s_type_%s_num'%(d,key[:1],str(i))].groupby(features['cate']).rank(ascending=False,method='min')\n",
    "            del features['cate']\n",
    "        pickle.dump(features, open(dump_path, 'wb+'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据集 features公共的 特征 如对6种行为的统计 如 user_id_brand,\n",
    "def get_public_double_time_features(key1,key2,start_date,end_date,d):\n",
    "    if key2=='sku_id':\n",
    "        tt='sku'\n",
    "    else:\n",
    "        tt='brand'\n",
    "    dump_path = r'cache/us/%s_%s_action_time_features_%s_%s.pkl' % (key1[:4],tt,start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        features = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        #u-s or u-b\n",
    "        #不统计 type=4 没有意义\n",
    "        dataset=get_actions_cate8(start_date,end_date)\n",
    "        features=None\n",
    "        #6种行为统计+时间\n",
    "        if key2=='sku_id':\n",
    "            t=dataset[[key1,key2,'brand','cate']]\n",
    "            t.drop_duplicates(inplace=True) #去重\n",
    "            #6种行为\n",
    "            for i in (1,2,3,5,6):\n",
    "                action=dataset[dataset.type==i][[key1,key2]]\n",
    "                action['%d_%s%s_type_%s_num'%(d,key1[:1],key2[:1],str(i))]=1\n",
    "                action=action.groupby([key1,key2]).agg('sum').reset_index()\n",
    "                if features is None:\n",
    "                    features=pd.merge(t,action,on=[key1,key2],how='left')\n",
    "                else:\n",
    "                    features=pd.merge(features,action,on=[key1,key2],how='left')\n",
    "            features=features.fillna(0)\n",
    "            if d==15:\n",
    "                #浏览次数，加购次数,删购物车次数，收藏，点击\n",
    "                #一件商品只能收藏一次，故没有排序必要\n",
    "                for i in (1,6):\n",
    "                    #us在ud中的排序\n",
    "                    features['%d_%s%s_type_%s_num_b_rank'%(d,key1[:1],key2[:1],str(i))]=features.groupby(['user_id','brand'])['%d_%s%s_type_%s_num'%(d,key1[:1],key2[:1],str(i))].rank(ascending=False,method='min')\n",
    "                    #us在uc中的排序\n",
    "                    features['%d_%s%s_type_%s_num_c_rank'%(d,key1[:1],key2[:1],str(i))]=features.groupby(['user_id','cate'])['%d_%s%s_type_%s_num'%(d,key1[:1],key2[:1],str(i))].rank(ascending=False,method='min')\n",
    "            del features['brand']\n",
    "            del features['cate']\n",
    "        if key2=='brand':\n",
    "            t=dataset[[key1,key2,'cate']]\n",
    "            t.drop_duplicates(inplace=True) #去重\n",
    "            #6种行为\n",
    "            for i in (1,2,3,5,6):\n",
    "                action=dataset[dataset.type==i][[key1,key2]]\n",
    "                action['%d_%s%s_type_%s_num'%(d,key1[:1],key2[:1],str(i))]=1\n",
    "                action=action.groupby([key1,key2]).agg('sum').reset_index()\n",
    "                if features is None:\n",
    "                    features=pd.merge(t,action,on=[key1,key2],how='left')\n",
    "                else:\n",
    "                    features=pd.merge(features,action,on=[key1,key2],how='left')\n",
    "            features=features.fillna(0)\n",
    "            if d==15:\n",
    "                #浏览次数，加购次数,删购物车次数，收藏，点击\n",
    "                for i in (1,2,3,5,6):\n",
    "                    features['%d_%s%s_type_%s_num_c_rank'%(d,key1[:1],key2[:1],str(i))]=features.groupby(['user_id','cate'])['%d_%s%s_type_%s_num'%(d,key1[:1],key2[:1],str(i))].rank(ascending=False,method='min')\n",
    "            del features['cate']\n",
    "        pickle.dump(features, open(dump_path, 'wb+'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取商品独特的信息 如 交互的人数 \n",
    "def get_product_unique_time_features(start_date,end_date,d):\n",
    "    dump_path = r'cache/us/product_unique_time_features_%s_%s.pkl' % (start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        features = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        dataset=get_actions_cate8(start_date,end_date)\n",
    "        features=None\n",
    "        #6种行为统计+时间\n",
    "        t=dataset[['sku_id','brand','cate']]\n",
    "        t.drop_duplicates(inplace=True) #去重\n",
    "        #交互人数\n",
    "        tp=dataset[['sku_id','user_id','type']]\n",
    "        tp.drop_duplicates(inplace=True)\n",
    "        for i in (1,2,3,4,5,6):\n",
    "            tt=tp[tp.type==i][['sku_id']]\n",
    "            tt['%d_s_type_%s_u_num'%(d,str(i))]=1\n",
    "            tt=tt.groupby(['sku_id']).agg('sum').reset_index()\n",
    "            if features is None:\n",
    "                features=pd.merge(t,tt,on='sku_id',how='left')\n",
    "            else:\n",
    "                features=pd.merge(features,tt,on='sku_id',how='left')\n",
    "        features=features.fillna(0)\n",
    "        if d==60:\n",
    "            for i in (1,2,3,4,5,6):\n",
    "                features['%d_s_type_%s_u_num_c_rank'%(d,str(i))]=features['%d_s_type_%s_u_num'%(d,str(i))].groupby(features['cate']).rank(ascending=False,method='min')\n",
    "                features['%d_s_type_%s_u_num_b_rank'%(d,str(i))]=features['%d_s_type_%s_u_num'%(d,str(i))].groupby(features['brand']).rank(ascending=False,method='min')\n",
    "        del features['cate']\n",
    "        del features['brand']\n",
    "        pickle.dump(features, open(dump_path, 'wb+'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取商品全局特征信息 如 \n",
    "def get_product_whole_features(start_date,end_date):\n",
    "    dump_path = r'cache/us/product_whole_features_%s_%s.pkl' % (start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        actions = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        feature = ['sku_id', 'product_1_convern', 'product_2_convern', 'product_3_convern',\n",
    "                   'product_5_convern', 'product_6_convern', 'product_action_convern']\n",
    "        actions = get_actions_cate8(start_date, end_date)\n",
    "        #最后一次交互/购买时间与考察日的差值\n",
    "        tt=actions[['sku_id','time']]\n",
    "        #最早交互天数\n",
    "        t1=tt.groupby('sku_id').agg('min').reset_index()\n",
    "        t1.rename(columns={'time':'sku_max_action_time'},inplace=True)\n",
    "        t1['sku_max_action_time']=t1.sku_max_action_time.astype('str').apply(lambda x:time_differ_days(end_date,x))\n",
    "        #最晚交互天数\n",
    "        t2=tt.groupby('sku_id').agg('max').reset_index()\n",
    "        t2.rename(columns={'time':'sku_min_action_time'},inplace=True)\n",
    "        t2['sku_min_action_time']=t2.sku_min_action_time.astype('str').apply(lambda x:time_differ_days(end_date,x))\n",
    "\n",
    "        #最早一次交互/购买时间与考察日的差值\n",
    "        #购买日期\n",
    "        ts=actions[actions.type==4][['sku_id','time']]\n",
    "        t3=ts.groupby('sku_id').agg('max').reset_index()\n",
    "        t3.rename(columns={'time':'sku_min_buy_time'},inplace=True)\n",
    "\n",
    "        t3['sku_min_buy_time']=t3.sku_min_buy_time.astype('str').apply(lambda x:time_differ_days(end_date,x))\n",
    "\n",
    "        t4=ts.groupby('sku_id').agg('min').reset_index()\n",
    "        t4.rename(columns={'time':'sku_max_buy_time'},inplace=True)\n",
    "        t4['sku_max_buy_time']=t4.sku_max_buy_time.astype('str').apply(lambda x:time_differ_days(end_date,x))\n",
    "\n",
    "        df = pd.get_dummies(actions['type'], prefix='action')\n",
    "        actions = pd.concat([actions['sku_id'], df], axis=1)\n",
    "        actions = actions.groupby(['sku_id'], as_index=False).sum()\n",
    "        actions['product_1_convern'] = actions['action_4'] / actions['action_1']\n",
    "        actions['product_2_convern'] = actions['action_4'] / actions['action_2']\n",
    "        actions['product_3_convern'] = actions['action_4'] / actions['action_3']\n",
    "        actions['product_5_convern'] = actions['action_4'] / actions['action_5']\n",
    "        actions['product_6_convern'] = actions['action_4'] / actions['action_6']\n",
    "        actions['product_action_convern'] = actions[feature[1:6]].sum(axis = 1)\n",
    "        actions = actions[feature]\n",
    "        actions=pd.merge(actions,t1,on='sku_id',how='left')\n",
    "        actions=pd.merge(actions,t2,on='sku_id',how='left')\n",
    "        actions=pd.merge(actions,t3,on='sku_id',how='left')\n",
    "        actions=pd.merge(actions,t4,on='sku_id',how='left')\n",
    "        actions['sku_min_action_time']=actions['sku_min_action_time'].replace(np.nan,-1)\n",
    "        actions['sku_max_action_time']=actions['sku_max_action_time'].replace(np.nan,-1)\n",
    "        actions['sku_max_buy_time']=actions['sku_max_buy_time'].replace(np.nan,-1)\n",
    "        actions['sku_min_buy_time']=actions['sku_min_buy_time'].replace(np.nan,-1)\n",
    "        actions=actions.fillna(0)\n",
    "        #只有近15天的交互信息判定为新产品\n",
    "        actions['new_sku_yes_no']=actions.sku_max_action_time.apply(lambda x:0 if x>=15 else 1)\n",
    "        pickle.dump(actions, open(dump_path, 'wb+'))\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user-product 交互特征\n",
    "def get_user_product_time_features(start_date,end_date,d):\n",
    "    dump_path = r'cache/us/user_product_time_features_%s_%s.pkl' % (start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        features = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        features=get_actions_cate8(start_date,end_date)\n",
    "        t=features[['user_id','sku_id','brand','cate']]\n",
    "        t.drop_duplicates(inplace=True) #去重\n",
    "\n",
    "        #交互天数\n",
    "        t1=features[['user_id','sku_id','time']]\n",
    "        t1['date'] = t1['time'].apply(lambda x:x[:10])\n",
    "        t1 = t1[['user_id','sku_id','date']].drop_duplicates()\n",
    "        t1 = t1.groupby(['user_id','sku_id'],as_index=False)['date'].count()\n",
    "        t1.rename(columns = {'date': '%d_us_active_days' % d},inplace=True)\n",
    "\n",
    "        t2=features[['user_id','sku_id','time']]\n",
    "        #交互分钟数\n",
    "        t2['minute'] = t2['time'].apply(lambda x:x[:16])\n",
    "        t2 = t2[['user_id','sku_id','minute']].drop_duplicates()\n",
    "        t2 = t2.groupby(['user_id','sku_id'],as_index=False)['minute'].count()\n",
    "        t2.rename(columns = {'minute': '%d_us_active_minutes' % d},inplace=True)\n",
    "\n",
    "        #交互时间\n",
    "        t4=features[['user_id','sku_id','time']]\n",
    "        t5 = t4.sort_values(['user_id','sku_id', 'time']) # 先按照uid和time排一下序\n",
    "        t5['time']=pd.to_datetime(t5['time'])\n",
    "        t5['pre'] = t5['time'].shift(1)\n",
    "        t5['uid0'] = t5['user_id'].shift(1)\n",
    "        t5['sid0'] = t5['sku_id'].shift(1)\n",
    "        t5['minutes_time'] = (t5['time'] - t5['pre']).map(lambda x:x/np.timedelta64(1, 'm'))\n",
    "        t6 = t5[(t5['user_id'] == t5['uid0'])&(t5['sku_id'] == t5['sid0'])]\n",
    "        t6=t6[['user_id','sku_id','minutes_time']]\n",
    "        t7=t6[t6.minutes_time<5.0][['user_id','sku_id','minutes_time']]\n",
    "        t7=t7.groupby(['user_id','sku_id']).agg('sum').reset_index()\n",
    "        t7.rename(columns = {'minutes_time': '%d_us_minutes_time' % d},inplace=True)\n",
    "        #合并\n",
    "        features=pd.merge(t,t1,on=['user_id','sku_id'],how='left')\n",
    "        features=pd.merge(features,t2,on=['user_id','sku_id'],how='left')\n",
    "        features=pd.merge(features,t7,on=['user_id','sku_id'],how='left')\n",
    "        features=features.replace(np.nan,0)\n",
    "        #交互时间排序特征\n",
    "        if d==60:\n",
    "            features['%d_us_active_days_rank' % d]=features['%d_us_active_days' % d].groupby(features['user_id']).rank(ascending=False,method='min')\n",
    "            features['%d_us_active_minutes_rank' % d]=features['%d_us_active_minutes' % d].groupby(features['user_id']).rank(ascending=False,method='min')\n",
    "            features['%d_us_minutes_time_rank' % d]=features['%d_us_minutes_time' % d].groupby(features['user_id']).rank(ascending=False,method='min')\n",
    "        del features['brand']\n",
    "        del features['cate']\n",
    "        pickle.dump(features, open(dump_path, 'wb+'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_product_whole_features(start_date,end_date):\n",
    "    dump_path = r'cache/us/user_product_whole_features_%s_%s.pkl' % (start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        features = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        #获取 用户商品交互时间的最早 最完时间\n",
    "         #用户商品交互时间\n",
    "        user_product=get_actions_cate8(start_date,end_date)\n",
    "        t=user_product[['user_id','sku_id']]\n",
    "        t.drop_duplicates(inplace=True) #去重\n",
    "        t2=user_product[['user_id','sku_id','time']]\n",
    "        #获取最晚时间，获取最早时间，求差值\n",
    "        t3=t2.groupby(['user_id','sku_id']).agg('max').reset_index()\n",
    "        t3['time']=t3.time.astype('str').apply(lambda x:time_differ_days(end_date,x))\n",
    "        t3.rename(columns={'time':'u_s_max_days'},inplace=True)\n",
    "\n",
    "        t4=t2.groupby(['user_id','sku_id']).agg('min').reset_index()\n",
    "        t4['time']=t4.time.astype('str').apply(lambda x:time_differ_days(end_date,x))\n",
    "        t4.rename(columns={'time':'u_s_min_days'},inplace=True)\n",
    "\n",
    "        t5=t2.groupby(['user_id','sku_id']).agg('max').reset_index()\n",
    "        t5['time']=t5.time.astype('str').apply(lambda x:diff_of_hours(end_date,x))\n",
    "        t5.rename(columns={'time':'u_s_min_hours'},inplace=True)\n",
    "\n",
    "        t6=t2.groupby(['user_id','sku_id']).agg('min').reset_index()\n",
    "        t6['time']=t6.time.astype('str').apply(lambda x:diff_of_hours(end_date,x))\n",
    "        t6.rename(columns={'time':'u_s_max_hours'},inplace=True)\n",
    "        \n",
    "        tt=user_product[['user_id','time']]\n",
    "        t7=user_product[['user_id','sku_id','time']]\n",
    "        t7.drop_duplicates(inplace=True)\n",
    "        #user_id max_time\n",
    "        t8=tt.groupby(['user_id']).agg('max').reset_index()\n",
    "        t8=pd.merge(t8,t7,on=['user_id','time'],how='left')\n",
    "        t8['u_last_see_sku']=1\n",
    "        del t8['time']\n",
    "        \n",
    "        t9=tt.groupby(['user_id']).agg('min').reset_index()\n",
    "        t9=pd.merge(t9,t7,on=['user_id','time'],how='left')\n",
    "        t9['u_first_see_sku']=1\n",
    "        del t9['time']\n",
    "        \n",
    "        features=pd.merge(t,t3,on=['user_id','sku_id'],how='left')\n",
    "        features=pd.merge(features,t4,on=['user_id','sku_id'],how='left')\n",
    "        features=pd.merge(features,t5,on=['user_id','sku_id'],how='left')\n",
    "        features=pd.merge(features,t6,on=['user_id','sku_id'],how='left')\n",
    "        features=features.fillna(-1)\n",
    "        features=pd.merge(features,t8,on=['user_id','sku_id'],how='left')\n",
    "        features=pd.merge(features,t9,on=['user_id','sku_id'],how='left')\n",
    "        features=features.fillna(0)\n",
    "        pickle.dump(features, open(dump_path, 'wb+'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user-brand 交互特征\n",
    "def get_user_brand_time_features(start_date,end_date,d):\n",
    "    dump_path = r'cache/us/user_brand_time_features_%s_%s.pkl' % (start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        features = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        features=get_actions_cate8(start_date,end_date)\n",
    "        t=features[['user_id','brand']]\n",
    "        t.drop_duplicates(inplace=True) #去重\n",
    "\n",
    "        #交互天数\n",
    "        t1=features[['user_id','brand','time']]\n",
    "        t1['date'] = t1['time'].apply(lambda x:x[:10])\n",
    "        t1 = t1[['user_id','brand','date']].drop_duplicates()\n",
    "        t1 = t1.groupby(['user_id','brand'],as_index=False)['date'].count()\n",
    "        t1.rename(columns = {'date': '%d_ub_active_days' % d},inplace=True)\n",
    "\n",
    "        t2=features[['user_id','brand','time']]\n",
    "        #交互分钟数\n",
    "        t2['minute'] = t2['time'].apply(lambda x:x[:16])\n",
    "        t2 = t2[['user_id','brand','minute']].drop_duplicates()\n",
    "        t2 = t2.groupby(['user_id','brand'],as_index=False)['minute'].count()\n",
    "        t2.rename(columns = {'minute': '%d_ub_active_minutes' % d},inplace=True)\n",
    "\n",
    "        #交互时间\n",
    "        t4=features[['user_id','brand','time']]\n",
    "        t5 = t4.sort_values(['user_id','brand', 'time']) # 先按照uid和time排一下序\n",
    "        t5['time']=pd.to_datetime(t5['time'])\n",
    "        t5['pre'] = t5['time'].shift(1)\n",
    "        t5['uid0'] = t5['user_id'].shift(1)\n",
    "        t5['sid0'] = t5['brand'].shift(1)\n",
    "        t5['minutes_time'] = (t5['time'] - t5['pre']).map(lambda x:x/np.timedelta64(1, 'm'))\n",
    "        t6 = t5[(t5['user_id'] == t5['uid0'])&(t5['brand'] == t5['sid0'])]\n",
    "        t6=t6[['user_id','brand','minutes_time']]\n",
    "        t7=t6[t6.minutes_time<5.0][['user_id','brand','minutes_time']]\n",
    "        t7=t7.groupby(['user_id','brand']).agg('sum').reset_index()\n",
    "        t7.rename(columns = {'minutes_time': '%d_ub_minutes_time' % d},inplace=True)\n",
    "\n",
    "        #合并\n",
    "        features=pd.merge(t,t1,on=['user_id','brand'],how='left')\n",
    "        features=pd.merge(features,t2,on=['user_id','brand'],how='left')\n",
    "        features=pd.merge(features,t7,on=['user_id','brand'],how='left')\n",
    "        features=features.replace(np.nan,0)\n",
    "\n",
    "        #交互时间排序特征\n",
    "        if d==15:\n",
    "            features['%d_ub_active_days_rank' % d]=features['%d_ub_active_days' % d].groupby(features['user_id']).rank(ascending=False,method='min')\n",
    "            features['%d_ub_active_minutes_rank' % d]=features['%d_ub_active_minutes' % d].groupby(features['user_id']).rank(ascending=False,method='min')\n",
    "            features['%d_ub_minutes_time_rank' % d]=features['%d_ub_minutes_time' % d].groupby(features['user_id']).rank(ascending=False,method='min')\n",
    "        #加权特征 相对来说 加购物车\n",
    "        pickle.dump(features, open(dump_path, 'wb+'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取商品全局特征信息 如 \n",
    "def get_brand_whole_features(start_date,end_date):\n",
    "    dump_path = r'cache/us/brand_whole_features_%s_%s.pkl' % (start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        actions = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        feature = ['brand', 'brand_1_convern', 'brand_2_convern', 'brand_3_convern',\n",
    "                   'brand_5_convern', 'brand_6_convern', 'brand_action_convern']\n",
    "        actions = get_actions_cate8(start_date, end_date)\n",
    "        df = pd.get_dummies(actions['type'], prefix='action')\n",
    "        actions = pd.concat([actions['brand'], df], axis=1)\n",
    "        actions = actions.groupby(['brand'], as_index=False).sum()\n",
    "        actions['brand_1_convern'] = actions['action_4'] / actions['action_1']\n",
    "        actions['brand_2_convern'] = actions['action_4'] / actions['action_2']\n",
    "        actions['brand_3_convern'] = actions['action_4'] / actions['action_3']\n",
    "        actions['brand_5_convern'] = actions['action_4'] / actions['action_5']\n",
    "        actions['brand_6_convern'] = actions['action_4'] / actions['action_6']\n",
    "        actions['brand_action_convern'] = actions[feature[1:6]].sum(axis = 1)\n",
    "        actions = actions[feature]\n",
    "        actions=actions.fillna(0)\n",
    "        actions=actions.replace(np.inf,0)\n",
    "        pickle.dump(actions, open(dump_path, 'wb+'))\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_whole_features(start_date,end_date):\n",
    "    dump_path = r'cache/us/user_whole_features_%s_%s.pkl' % (start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        features = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        dataset=get_actions_cate8(start_date,end_date)\n",
    "        t=dataset[['user_id']]\n",
    "        t.drop_duplicates(inplace=True) #去重\n",
    "         #是否购买过category8商品\n",
    "        ts=dataset[dataset.type==4][['user_id']]\n",
    "        ts['user_buy_cate8_yes_no']=1\n",
    "        ts.drop_duplicates(inplace=True)\n",
    "        features=pd.merge(t,ts,on='user_id',how='left')\n",
    "        features['user_buy_cate8_yes_no']=features['user_buy_cate8_yes_no'].replace(np.nan,0)\n",
    "        pickle.dump(features, open(dump_path, 'wb+'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取正样本\n",
    "def get_us_labels(start_date, end_date):\n",
    "    dump_path = r'cache/us/user_sku_labels_%s_%s.pkl' % (start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        actions = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        actions = get_actions_cate8(start_date, end_date)\n",
    "        actions = actions[actions['type'] == 4]\n",
    "        actions = actions[['user_id', 'sku_id']].drop_duplicates()\n",
    "        actions['us_label'] = 1\n",
    "        pickle.dump(actions, open(dump_path, 'wb+'))\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_u_labels(start_date, end_date):\n",
    "    dump_path = r'cache/us/user_labels_%s_%s.pkl' % (start_date[:10], end_date[:10])\n",
    "    if os.path.exists(dump_path):\n",
    "        actions = pickle.load(open(dump_path, 'rb+'))\n",
    "    else:\n",
    "        actions = get_actions_cate8(start_date, end_date)\n",
    "        actions = actions[actions['type'] == 4]\n",
    "        actions = actions[['user_id']].drop_duplicates()\n",
    "        actions['u_label'] = 1\n",
    "        pickle.dump(actions, open(dump_path, 'wb+'))\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_end_data 如11-15day 的特征为 10号之前\n",
    "def make_train_set(train_end_date,test_start_date,test_end_date):\n",
    "    start_date = \"2016-01-31\"                                                   #总时间                     \n",
    "    product = get_basic_product_feat()                                          # 商品基本信息\n",
    "    comment = get_comments(train_end_date)                                      # 商品评论信息 \n",
    "    user_whole_infor=get_user_whole_features(start_date,train_end_date)\n",
    "    brand_whole_infor=get_brand_whole_features(start_date,train_end_date)\n",
    "    product_whole_infor=get_product_whole_features(start_date,train_end_date)\n",
    "    user_product_whole_infor=get_user_product_whole_features(start_date,train_end_date)\n",
    "    us_labels = get_us_labels(test_start_date, test_end_date)                             # 获取label\n",
    "    u_labels=get_u_labels(test_start_date, test_end_date)  \n",
    "    actions = None                                                              # 用户商品交互行为次数特征\n",
    "    user_product_action=None                                                    \n",
    "    product_action=None\n",
    "    product_unique_action=None\n",
    "    user_brand_action=None\n",
    "    user_brand=None\n",
    "    #user_all_data=None\n",
    "    for i in (1, 2, 3, 5, 7, 10, 15,30,60):\n",
    "        start_days=datetime.strptime(train_end_date,'%Y-%m-%d %H:%M:%S')-timedelta(days=i)\n",
    "        start_days=start_days.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        if actions is None:\n",
    "            #user_notcate8_action=get_user_other_data_time_features(start_days, train_end_date, i)\n",
    "            #user_action = get_public_single_time_features('user_id',start_days, train_end_date, i)#6个\n",
    "            #user_unique_action=get_user_unique_time_features(start_days, train_end_date, i)#14\n",
    "            actions = get_public_double_time_features('user_id','sku_id',start_days,train_end_date,i)#6种行为统计\n",
    "            user_product_action = get_user_product_time_features(start_days,train_end_date,i)#2个交互时间，交互天数\n",
    "            \n",
    "            product_action=get_public_single_time_features('sku_id',start_days, train_end_date, i)\n",
    "            product_unique_action=get_product_unique_time_features(start_days, train_end_date, i)\n",
    "            user_brand=get_public_double_time_features('user_id','brand',start_days,train_end_date,i)#6种行为统计\n",
    "            user_brand_action = get_user_brand_time_features(start_days,train_end_date,i)#2个交互时间，交互天数\n",
    "        else:\n",
    "            if (i<61):\n",
    "                actions = pd.merge(get_public_double_time_features('user_id','sku_id',start_days,train_end_date,i), actions, how='left',\n",
    "                                    on=['user_id', 'sku_id'])\n",
    "                user_product_action = pd.merge(get_user_product_time_features(start_days,train_end_date,i), user_product_action, how='left',\n",
    "                                    on=['user_id', 'sku_id'])\n",
    "                product_action=pd.merge(get_public_single_time_features('sku_id',start_days, train_end_date, i),product_action,how='left',\n",
    "                                    on=['sku_id'])\n",
    "                product_unique_action=pd.merge(get_product_unique_time_features(start_days, train_end_date, i),product_unique_action,how='left',\n",
    "                                    on=['sku_id'])\n",
    "                if i==30:\n",
    "                    user_brand= pd.merge(get_public_double_time_features('user_id','brand',start_days,train_end_date,i), user_brand, how='left',\n",
    "                                       on=['user_id', 'brand'])\n",
    "                    user_brand_action= pd.merge(get_user_brand_time_features(start_days,train_end_date,i), user_brand_action, how='left',\n",
    "                                        on=['user_id', 'brand'])\n",
    "            else:\n",
    "                actions = pd.merge(actions,get_public_double_time_features('user_id','sku_id',start_days,train_end_date,i), how='left',\n",
    "                                    on=['user_id', 'sku_id'])\n",
    "                user_product_action = pd.merge(user_product_action,get_user_product_time_features(start_days,train_end_date,i), how='left',\n",
    "                                    on=['user_id', 'sku_id'])\n",
    "                product_action=pd.merge(product_action,get_public_single_time_features('sku_id',start_days, train_end_date, i),how='left',\n",
    "                                    on=['sku_id'])\n",
    "                product_unique_action=pd.merge(product_unique_action,get_product_unique_time_features(start_days, train_end_date, i),how='left',\n",
    "                                    on=['sku_id'])\n",
    "                user_brand= pd.merge(user_brand,get_public_double_time_features('user_id','brand',start_days,train_end_date,i),how='left',\n",
    "                                    on=['user_id', 'brand'])\n",
    "                user_brand_action= pd.merge(user_brand_action,get_user_brand_time_features(start_days,train_end_date,i),  how='left',\n",
    "                                    on=['user_id', 'brand'])\n",
    "                \n",
    "    #actions = pd.merge(actions, user,                     how='left', on='user_id')\n",
    "    actions = pd.merge(actions, user_whole_infor,          how='left', on='user_id')\n",
    "    #actions = pd.merge(actions, user_whole_notcate8_infor,how='left', on='user_id')\n",
    "    #actions = pd.merge(actions, user_whole_all_infor,     how='left', on='user_id')\n",
    "    #actions = pd.merge(actions, user_unique_action,       how='left', on='user_id')\n",
    "    #actions = pd.merge(actions, user_notcate8_action,     how='left', on='user_id')\n",
    "    #actions = pd.merge(actions, user_all_data,            how='left', on='user_id')#比例特征\n",
    "    \n",
    "    actions = pd.merge(actions, product,                  how='left', on='sku_id')\n",
    "    actions = pd.merge(actions, comment,                  how='left', on='sku_id')\n",
    "    actions = pd.merge(actions, product_whole_infor,      how='left', on='sku_id')\n",
    "    \n",
    "    actions = pd.merge(actions, product_action,           how='left', on='sku_id')\n",
    "    actions = pd.merge(actions, product_unique_action,    how='left', on='sku_id')\n",
    "    \n",
    "    actions = pd.merge(actions, user_product_whole_infor, how='left', on=['user_id', 'sku_id'])\n",
    "    actions = pd.merge(actions, user_product_action,      how='left', on=['user_id', 'sku_id']) \n",
    "    \n",
    "    actions = pd.merge(actions, brand_whole_infor,        how='left', on=['brand'])\n",
    "    actions = pd.merge(actions, user_brand,               how='left', on=['user_id','brand'])\n",
    "    actions = pd.merge(actions, user_brand_action,        how='left', on=['user_id','brand'])\n",
    "    \n",
    "    actions = pd.merge(actions, us_labels,                   how='left', on=['user_id', 'sku_id'])\n",
    "    actions = pd.merge(actions, u_labels,                    how='left', on=['user_id'])\n",
    "    actions['us_label'] = actions['us_label'].fillna(0)\n",
    "    actions['u_label'] = actions['u_label'].fillna(0)\n",
    "    actions = actions.fillna(0)\n",
    "    return actions, us_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test():\n",
    "    \n",
    "    data1, y_true1 = make_train_set('2016-04-11 00:00:00', '2016-04-11 00:00:00', '2016-04-16 00:00:00')\n",
    "    data1.to_csv('features/us/data1_features.csv',index=False,encoding='utf-8')\n",
    "    y_true1.to_csv('features/us/true1_features.csv',index=False,encoding='utf-8')\n",
    "    del data1\n",
    "    del y_true1\n",
    "    gc.collect()\n",
    "    print '1'\n",
    "    data0, y_true0 = make_train_set('2016-04-16 00:00:00', '2016-04-16 00:00:00', '2016-04-21 00:00:00')\n",
    "    data0.to_csv('features/us/data0_features.csv',index=False,encoding='utf-8')\n",
    "    y_true0.to_csv('features/us/true0_features.csv',index=False,encoding='utf-8')\n",
    "    del data0\n",
    "    del y_true0\n",
    "    gc.collect()\n",
    "    print '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admain/anaconda2/lib/python2.7/site-packages/pandas/util/decorators.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return func(*args, **kwargs)\n",
      "/home/admain/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/admain/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
